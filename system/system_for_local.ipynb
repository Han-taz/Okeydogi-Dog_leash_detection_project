{"cells":[{"cell_type":"markdown","id":"99a843f4-d39f-48a8-b220-7ba496659500","metadata":{"jp-MarkdownHeadingCollapsed":true,"tags":[],"id":"99a843f4-d39f-48a8-b220-7ba496659500"},"source":["# Local"]},{"cell_type":"code","execution_count":null,"id":"acfb8e8f-7281-4a00-a10e-69ae4427ac16","metadata":{"tags":[],"id":"acfb8e8f-7281-4a00-a10e-69ae4427ac16"},"outputs":[],"source":["#!pip install playsound==1.2.2"]},{"cell_type":"code","execution_count":null,"id":"04cab78c-1b83-4d81-b52e-2f033c8fc277","metadata":{"id":"04cab78c-1b83-4d81-b52e-2f033c8fc277"},"outputs":[],"source":["import cv2\n","import time\n","from playsound import playsound\n","import numpy as np\n","import torch\n","from tensorflow import keras\n","from PIL import Image\n","from keras.applications.resnet_v2 import preprocess_input\n","from datetime import datetime\n","from keras.utils import img_to_array\n","from keras.models import load_model\n","from datetime import datetime"]},{"cell_type":"markdown","id":"8f02094d-e94b-4aaa-8559-54ffbf23a684","metadata":{"tags":[],"id":"8f02094d-e94b-4aaa-8559-54ffbf23a684"},"source":["## Loading models"]},{"cell_type":"code","execution_count":null,"id":"bac145d4-aad6-42fb-a259-8e508772f9ca","metadata":{"jupyter":{"outputs_hidden":true},"tags":[],"id":"bac145d4-aad6-42fb-a259-8e508772f9ca"},"outputs":[],"source":["# YOLO\n","yolo_model = torch.hub.load('WongKinYiu/yolov7','custom','../object_detection/pt/0213_yolov7_epochs55.pt')"]},{"cell_type":"code","execution_count":null,"id":"f6591047-0a52-4c3f-8c87-cb23391d08d6","metadata":{"id":"f6591047-0a52-4c3f-8c87-cb23391d08d6"},"outputs":[],"source":["# YOLO settings\n","yolo_model.conf=0.7\n","yolo_model.classes=27\n","yolo_model.iou=0.6"]},{"cell_type":"code","execution_count":null,"id":"47029738-d700-4e96-a16c-c9b36fc1f5c8","metadata":{"id":"47029738-d700-4e96-a16c-c9b36fc1f5c8","outputId":"0cfa3365-d008-4f87-a703-b7a092223c0a"},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"]},{"name":"stderr","output_type":"stream","text":["No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"]}],"source":["# CNN\n","new_model = load_model(\"../binary_classification/models/0217_aug_flip\")\n","idx_to_classes = {0: 'with-leash', 1: 'without-leash'}"]},{"cell_type":"markdown","id":"f5f6f985-eb36-46be-bd1b-257f8646519e","metadata":{"tags":[],"id":"f5f6f985-eb36-46be-bd1b-257f8646519e"},"source":["## FUNCTIONS"]},{"cell_type":"code","execution_count":null,"id":"66690ad1-40c7-4284-8dd9-2cb56b3f5838","metadata":{"tags":[],"id":"66690ad1-40c7-4284-8dd9-2cb56b3f5838"},"outputs":[],"source":["def pred(img_path,probs=False):\n","    #print('-'*20)\n","    #Image.open(img_path).convert('RGB').resize((224,224)).show()\n","    img = keras.preprocessing.image.load_img(img_path, target_size=(224,224))\n","    array = keras.preprocessing.image.img_to_array(img) #(224,224,3)\n","    array = np.expand_dims(array, axis=0)  #(1, 224, 224, 3)\n","    img_array = preprocess_input(array)\n","    preds = new_model.predict(img_array)\n","    if probs:\n","        print('with vs without :',preds)\n","    return idx_to_classes[np.argmax(preds)]"]},{"cell_type":"code","execution_count":null,"id":"451a7da7-e522-4781-8528-471c459c805f","metadata":{"tags":[],"id":"451a7da7-e522-4781-8528-471c459c805f"},"outputs":[],"source":["def system(IMAGE_FILE):\n","    results=yolo_model(IMAGE_FILE)\n","    pos_lists = results.pandas().xyxy\n","    pos_df = pos_lists[0]\n","    dogs=pos_df[['xmin', 'ymin', 'xmax', 'ymax']]\n","    confidences=pos_df['confidence']\n","    for i in range(len(dogs)):\n","        dog=dogs.iloc[i].tolist()\n","        dog=list(map(int,dog))\n","        conf=confidences.iloc[i]\n","        xmin,ymin,xmax,ymax=dog\n","\n","        frame=Image.open(IMAGE_FILE).convert('RGB')\n","        name=IMAGE_FILE.split('.')[1].split('/')[-1]\n","        margin=20\n","        crop=frame.crop((max(xmin-margin,0),max(ymin-margin,0),min(xmax+margin,frame.width),min(ymax+margin,frame.height)))\n","\n","        crop.save(name+'_crop.jpg')\n","        pred(name+'_crop.jpg',probs=True)"]},{"cell_type":"markdown","id":"45e0e5d6-e4b6-4ec0-b288-e21d38c2f5e2","metadata":{"jp-MarkdownHeadingCollapsed":true,"tags":[],"id":"45e0e5d6-e4b6-4ec0-b288-e21d38c2f5e2"},"source":["## REAL"]},{"cell_type":"code","execution_count":null,"id":"aab291bd-22e8-44ae-ba9b-5b5032bfa8e6","metadata":{"id":"aab291bd-22e8-44ae-ba9b-5b5032bfa8e6","outputId":"c003b1e4-3d7d-4260-ece5-2e3e58071948"},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 126ms/step\n","with vs without : [[    0.99954  0.00046251]]\n"]}],"source":["system('../test_images/jin_with.jpg')"]},{"cell_type":"code","execution_count":null,"id":"f08cc959-2ea8-40fd-9025-f5570f8d9b56","metadata":{"id":"f08cc959-2ea8-40fd-9025-f5570f8d9b56","outputId":"0e482198-565b-4066-8397-dd7f1747ac65"},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 111ms/step\n","with vs without : [[  0.0082826     0.99172]]\n"]}],"source":["system('../test_images/jin_without_noclothes.jpg')"]},{"cell_type":"code","execution_count":null,"id":"35cb90e4-83c3-4ffd-afa2-5c7d94904f54","metadata":{"id":"35cb90e4-83c3-4ffd-afa2-5c7d94904f54","outputId":"decc8007-6802-4fec-9b25-f32964743474"},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 123ms/step\n","with vs without : [[  0.0010069     0.99899]]\n"]}],"source":["system('../test_images/jin_without.jpg')"]},{"cell_type":"markdown","id":"344c1529-1b61-4ff4-bf8d-21e2a4a0887b","metadata":{"jp-MarkdownHeadingCollapsed":true,"tags":[],"id":"344c1529-1b61-4ff4-bf8d-21e2a4a0887b"},"source":["# System ver"]},{"cell_type":"code","execution_count":null,"id":"a9999fa9-a7f1-406b-bb23-a80bec562764","metadata":{"id":"a9999fa9-a7f1-406b-bb23-a80bec562764"},"outputs":[],"source":["###################### environments #############################\n","import cv2\n","import time\n","from playsound import playsound\n","import numpy as np\n","import torch\n","from tensorflow import keras\n","from PIL import Image\n","from keras.applications.resnet_v2 import preprocess_input\n","from keras.utils import img_to_array\n","from keras.models import load_model\n","from datetime import datetime"]},{"cell_type":"code","execution_count":null,"id":"73d6b495-c4b8-42b6-b37d-361a38e563a9","metadata":{"jupyter":{"outputs_hidden":true},"tags":[],"id":"73d6b495-c4b8-42b6-b37d-361a38e563a9"},"outputs":[],"source":["################### YOLO model ##########################\n","print('yolo-start')\n","playsound(\"./sound/yolo.wav\")\n","yolo_model = torch.hub.load('WongKinYiu/yolov7','custom','../object_detection/pt/0213_yolov7_epochs55.pt')\n","yolo_model.conf=0.7 # threshold\n","yolo_model.classes=27\n","yolo_model.iou=0.6"]},{"cell_type":"code","execution_count":null,"id":"8d396627-14d6-4755-9c16-b41ef9b90540","metadata":{"id":"8d396627-14d6-4755-9c16-b41ef9b90540","outputId":"ecff1a6a-006f-4035-c376-997c95f5e7cd"},"outputs":[{"name":"stdout","output_type":"stream","text":["cnn-start\n","WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"]},{"name":"stderr","output_type":"stream","text":["No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"]}],"source":["################### CNN model ############################\n","print('cnn-start')\n","playsound(\"./sound/cnn.wav\")\n","model = load_model(\"./0217_aug_flip\")\n","idx_to_classes = {0: 'with-leash', 1: 'without-leash'}"]},{"cell_type":"code","execution_count":null,"id":"8c3b800f-f1ab-467e-8253-2df8cb7240c4","metadata":{"id":"8c3b800f-f1ab-467e-8253-2df8cb7240c4"},"outputs":[],"source":["def pred(img_path,probs=False):\n","    img = keras.preprocessing.image.load_img(img_path, target_size=(224,224))\n","    array = keras.preprocessing.image.img_to_array(img) #(224,224,3)\n","    array = np.expand_dims(array, axis=0)  #(1, 224, 224, 3)\n","    img_array = preprocess_input(array)\n","    preds = model.predict(img_array)\n","    if probs:\n","        print('with vs without :',preds)\n","    return idx_to_classes[np.argmax(preds)]"]},{"cell_type":"code","execution_count":null,"id":"b727b883-e3aa-4fc0-b54c-813c5a8f7e53","metadata":{"id":"b727b883-e3aa-4fc0-b54c-813c5a8f7e53"},"outputs":[],"source":["def capture():   \n","    while True:\n","        webcam=cv2.VideoCapture(0)\n","        while True:\n","            ret,frame=webcam.read()\n","            results=yolo_model(frame,size=224)\n","            pos_lists = results.pandas().xyxy\n","            pos_df = pos_lists[0]\n","            dogs=pos_df[['xmin', 'ymin', 'xmax', 'ymax']]\n","            confidences=pos_df['confidence']\n","            if len(dogs)>=1:\n","                break\n","        for i in range(len(dogs)):\n","            dog=dogs.iloc[i].tolist()\n","            dog=list(map(int,dog))\n","            conf=confidences.iloc[i]\n","            \n","            now=datetime.now()\n","            name = now.strftime('%y%m%d%H%M%S') + '.jpg'\n","            xmin,ymin,xmax,ymax=dog\n","            margin=20\n","            height=frame.shape[0]\n","            width=frame.shape[1]\n","            cv2.imwrite(name,frame[max(ymin-margin,0):min(ymax+margin,height),max(xmin-margin,0):min(xmax+margin,width),:])\n","            ret=pred(name,probs=True)\n","            if(ret=='without-leash'):\n","                playsound('./sound/alert_final.wav')\n","        webcam.release()\n","        time.sleep(1)\n","    webcam.release()"]},{"cell_type":"code","execution_count":null,"id":"6e9cc25a-9762-47a6-9420-689770b3e205","metadata":{"jupyter":{"outputs_hidden":true},"tags":[],"id":"6e9cc25a-9762-47a6-9420-689770b3e205"},"outputs":[],"source":["playsound(\"./sound/execute.wav\")\n","capture()"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}